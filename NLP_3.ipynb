{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representando texto - Capturando semántica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalmente cualquier tarea de PLN, comienza representando el texto en alguna forma numérica. En este capítulo se abordará diferentes caminos para hacerlo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el presente capítulo se incluirán los siguiente tópicos:\n",
    "* Poner documentos en una bolsa de palabras\n",
    "* Construcción del modelo N-gram\n",
    "* Representando textos con TF-IDF\n",
    "* Uso de incrustaciones de palabras\n",
    "* Entrenando su propio modelo de incrustaciones\n",
    "* Representar frases - frasea2vec\n",
    "* Uso de BERT en lugar de incrustaciones de palabras\n",
    "* Comenzando con la búsqueda semántica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poner documentos en una bolsa de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método de la bolsa de palabras utiliza un texto de entrenamiento que le proporciona una lista de palabras que debe considerar. Al codificar nuevas oraciones, cuenta el número de ocurrencias que cada palabra hace en el documento, y el vector final incluye esos conteos para cada palabra en el vocabulario. Esta representación puede entonces ser alimentada en un algoritmo de aprendizaje automático."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las funciones a trabajar:\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(filename):\n",
    "    file = open(filename, \"r\", encoding=\"utf-8\") \n",
    "    return file.read()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return text\n",
    "\n",
    "def divide_into_sentences_nltk(text):\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función get_sentences, que leerá en el archivo de texto,\n",
    "#preprocesar el texto, y dividirlo en oraciones:\n",
    "def get_sentences(filename):\n",
    "    text = read_text_file(filename)\n",
    "    text = preprocess_text(text)\n",
    "    sentences = divide_into_sentences_nltk(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una función que devolverá el vectorizador y la matriz final:\n",
    "def create_vectorizer(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    return (vectorizer, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora, utilice las funciones antes mencionadas en el archivo pv.txt:\n",
    "sentences = get_sentences(\"pv.txt\")\n",
    "(vectorizer, X) = create_vectorizer(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 33)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 37)\t1\n",
      "  (0, 40)\t1\n",
      "  (0, 29)\t1\n",
      "  (0, 49)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 50)\t1\n",
      "  (0, 21)\t2\n",
      "  (0, 28)\t1\n",
      "  (0, 41)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 25)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 34)\t1\n",
      "  (0, 48)\t1\n",
      "  (0, 32)\t1\n",
      "  (1, 33)\t1\n",
      "  (1, 29)\t1\n",
      "  (1, 25)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 47)\t3\n",
      "  :\t:\n",
      "  (2, 46)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 26)\t2\n",
      "  (2, 5)\t2\n",
      "  (2, 55)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 53)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 23)\t1\n",
      "  (3, 2)\t2\n",
      "  (3, 34)\t1\n",
      "  (3, 26)\t1\n",
      "  (3, 45)\t1\n",
      "  (3, 52)\t1\n",
      "  (3, 43)\t1\n",
      "  (3, 56)\t1\n",
      "  (3, 35)\t1\n",
      "  (3, 27)\t1\n",
      "  (3, 11)\t1\n",
      "  (3, 3)\t2\n",
      "  (3, 39)\t1\n",
      "  (3, 38)\t1\n",
      "  (3, 20)\t1\n"
     ]
    }
   ],
   "source": [
    "# Ahora vamos a imprimir la representación matricial del texto:\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz resultante es un objeto scipy.sparse.csr.csr_matrix. También se puede convertir en un objeto numpy.matrixlib.defmatrix.matrix, donde cada oración es un vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseX = X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 2 0 0 0 1 0 0 1 1 0 0 1 1 1 0\n",
      "  0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 2 0 1 0 0\n",
      "  1 0 0 0 0 0 2 0 1 0 0 3 0 0 0 1 0 0 0 0 0]\n",
      " [1 1 0 0 0 2 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 2 0 0 1 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 0]\n",
      " [0 0 2 2 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1\n",
      "  0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(denseX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ac', 'an', 'and', 'are', 'array', 'as', 'basic', 'be', 'cells', 'changes', 'circuit', 'circuits', 'coupled', 'dc', 'developed', 'directly', 'effects', 'environment', 'equations', 'gui', 'here', 'in', 'including', 'inverter', 'irradiation', 'is', 'load', 'matching', 'matlab', 'model', 'new', 'of', 'paper', 'photovoltaic', 'presented', 'proper', 'pv', 'pva', 'results', 'simulated', 'simulation', 'simulink', 'solar', 'studies', 'temperature', 'test', 'tested', 'the', 'this', 'to', 'used', 'using', 'validation', 'via', 'was', 'well', 'with']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cdani\\anaconda3\\envs\\NLP_cap3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Podemos ver todas las palabras que se usaron en el conjunto de documentos:\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La función transform espera una lista de documentos,\n",
    "# por lo que crearemos una nueva lista con la oración como único elemento:\n",
    "new_sentence = \"I had seen little of Holmes lately.\"\n",
    "new_sentence_vector = vectorizer.transform([new_sentence])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 31)\t1\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Ahora podemos imprimir las matrices dispersas y densas de esta nueva frase:\n",
    "print(new_sentence_vector)\n",
    "print(new_sentence_vector.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hay mas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(filename):\n",
    "    file = open(filename, \"r\", encoding=\"utf-8\") \n",
    "    return file.read()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return text\n",
    "\n",
    "def divide_into_sentences_nltk(text):\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"sherlock_holmes_1.txt\"\n",
    "sherlock_holmes_text = read_text_file(filename)\n",
    "sherlock_holmes_text = preprocess_text(sherlock_holmes_text)\n",
    "sentences = divide_into_sentences_nltk(sherlock_holmes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una nueva clase de vectorizador. Esta vez, utilice el argumento stop_words:\n",
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilice el objeto vectorizador para obtener la matriz:\n",
    "X = vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_the_', 'abhorrent', 'actions', 'adjusted', 'adler', 'admirable', 'admirably', 'admit', 'akin', 'balanced', 'cold', 'crack', 'delicate', 'distracting', 'disturbing', 'doubt', 'drawing', 'dubious', 'eclipses', 'emotion', 'emotions', 'excellent', 'eyes', 'factor', 'false', 'felt', 'finely', 'gibe', 'grit', 'heard', 'high', 'holmes', 'instrument', 'introduce', 'intrusions', 'irene', 'late', 'lenses', 'love', 'lover', 'machine', 'memory', 'men', 'mental', 'mention', 'mind', 'motives', 'nature', 'observer', 'observing', 'particularly', 'passions', 'perfect', 'placed', 'position', 'power', 'precise', 'predominates', 'questionable', 'reasoner', 'reasoning', 'results', 'save', 'seen', 'seldom', 'sensitive', 'sex', 'sherlock', 'sneer', 'softer', 'spoke', 'strong', 'temperament', 'things', 'throw', 'trained', 'veil', 'woman', 'world']\n"
     ]
    }
   ],
   "source": [
    "# Podemos imprimir el vocabulario así:\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos aplicar el nuevo vectorizador a una de las oraciones del conjunto original y usar la función build_analyzer para ver más claramente el análisis de la oración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = \"And yet there was but one woman to him, \\\n",
    "                and that woman was the late Irene Adler, of dubious and questionable memory.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['woman', 'woman', 'late', 'irene', 'adler', 'dubious', 'questionable', 'memory']\n"
     ]
    }
   ],
   "source": [
    "new_sentence_vector = vectorizer.transform([new_sentence])\n",
    "analyze = vectorizer.build_analyzer()\n",
    "print(analyze(new_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 35)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 41)\t1\n",
      "  (0, 58)\t1\n",
      "  (0, 77)\t2\n"
     ]
    }
   ],
   "source": [
    "# Podemos imprimir el escaso vector de la frase así:\n",
    "print(new_sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
