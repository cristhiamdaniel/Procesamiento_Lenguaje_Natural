{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jugando con la gramática"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temas a tratar:\n",
    "\n",
    "* Contando sustantivos (plurales y singulares).\n",
    "* Obtener el análisis de dependencia.\n",
    "* División de frases en cláusulas.\n",
    "* Extracción de trozos sustantivos.\n",
    "* Entidades extractoras y relaciones.\n",
    "* Extraer sujetos y objetos de la oración.\n",
    "* Encontrar referencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contar sustantivos - sustantivos plurales y singulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importando los módulos necesarios\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_nltk(text):\n",
    "    return nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "def pos_tag_nltk(text):\n",
    "    words = tokenize_nltk(text)\n",
    "    words_with_pos = nltk.pos_tag(words)\n",
    "    return words_with_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura del texto en inglés\n",
    "filename = \"pv.txt\"\n",
    "file = open(filename, \"r\", encoding=\"utf-8\")\n",
    "text = file.read()\n",
    "text = text.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('photovoltaic', 'JJ'), ('array', 'NN'), ('(', '('), ('PVA', 'NNP'), (')', ')'), ('simulation', 'NN'), ('model', 'NN'), ('to', 'TO'), ('be', 'VB'), ('used', 'VBN'), ('in', 'IN'), ('Matlab-Simulink', 'NNP'), ('GUI', 'NNP'), ('environment', 'NN'), ('is', 'VBZ'), ('developed', 'VBN'), ('and', 'CC'), ('presented', 'VBN'), ('in', 'IN'), ('this', 'DT'), ('paper', 'NN'), ('.', '.'), ('The', 'DT'), ('model', 'NN'), ('is', 'VBZ'), ('developed', 'VBN'), ('using', 'VBG'), ('basic', 'JJ'), ('circuit', 'NN'), ('equations', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('photovoltaic', 'NN'), ('(', '('), ('PV', 'NNP'), (')', ')'), ('solar', 'VBP'), ('cells', 'NNS'), ('including', 'VBG'), ('the', 'DT'), ('effects', 'NNS'), ('of', 'IN'), ('solar', 'JJ'), ('irradiation', 'NN'), ('and', 'CC'), ('temperature', 'NN'), ('changes', 'NNS'), ('.', '.'), ('The', 'DT'), ('new', 'JJ'), ('model', 'NN'), ('was', 'VBD'), ('tested', 'VBN'), ('using', 'VBG'), ('a', 'DT'), ('directly', 'RB'), ('coupled', 'VBN'), ('dc', 'NN'), ('load', 'NN'), ('as', 'RB'), ('well', 'RB'), ('as', 'IN'), ('ac', 'JJ'), ('load', 'NN'), ('via', 'IN'), ('an', 'DT'), ('inverter', 'NN'), ('.', '.'), ('Test', 'NNP'), ('and', 'CC'), ('validation', 'NN'), ('studies', 'NNS'), ('with', 'IN'), ('proper', 'JJ'), ('load', 'NN'), ('matching', 'VBG'), ('circuits', 'NNS'), ('are', 'VBP'), ('simulated', 'VBN'), ('and', 'CC'), ('results', 'NNS'), ('are', 'VBP'), ('presented', 'VBN'), ('here', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Hacer parte del etiquetado del habla\n",
    "words_with_pos = pos_tag_nltk(text)\n",
    "print(words_with_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función get_nouns, para filtrar los sustantivos:\n",
    "def get_nouns(words_with_pos):\n",
    "    noun_set = [\"NN\", \"NNS\"]\n",
    "    nouns = [word for word in words_with_pos if  word[1] in noun_set]\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('array', 'NN'), ('simulation', 'NN'), ('model', 'NN'), ('environment', 'NN'), ('paper', 'NN'), ('model', 'NN'), ('circuit', 'NN'), ('equations', 'NNS'), ('photovoltaic', 'NN'), ('cells', 'NNS'), ('effects', 'NNS'), ('irradiation', 'NN'), ('temperature', 'NN'), ('changes', 'NNS'), ('model', 'NN'), ('dc', 'NN'), ('load', 'NN'), ('load', 'NN'), ('inverter', 'NN'), ('validation', 'NN'), ('studies', 'NNS'), ('load', 'NN'), ('circuits', 'NNS'), ('results', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "# Ejecutamos la función anterior en la lista de palabras etiquetadas con POS:\n",
    "nouns = get_nouns(words_with_pos)\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para determinar si un sustantivo es singular o plural, tenemos dos opciones. La primera opción es usar las etiquetas NLTK, donde NN indica un sustantivo singular y NNS indica un sustantivo plural. <br>\n",
    "\n",
    "La siguiente función utiliza las etiquetas NLTK y devuelve True si el sustantivo de entrada es plural:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_plural_nltk(noun_info):\n",
    "    pos = noun_info[1]\n",
    "    if (pos == \"NNS\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_plural_nltk(('equations', 'NNS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_plural_nltk(('photovoltaic', 'NN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La otra opción es usar la clase WordNetLemmatizer en el paquete nltk.stem. La siguiente función devuelve True si el sustantivo es plural:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_plural_wn(noun):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemma = wnl.lemmatize(noun, 'n')\n",
    "    plural = True if noun is not lemma else False\n",
    "    return plural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_plural_wn('equations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_plural_wn('photovoltaic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La siguiente función cambiará un sustantivo singular en plural:\n",
    "def get_plural(singular_noun):\n",
    "    p = inflect.engine()\n",
    "    return p.plural(singular_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'photovoltaics'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_plural('photovoltaic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La siguiente función cambiará un sustantivo singular en plural:\n",
    "def get_singular(plural_noun):\n",
    "    p = inflect.engine()\n",
    "    plural = p.singular_noun(plural_noun)\n",
    "    if (plural):\n",
    "        return plural\n",
    "    else:\n",
    "        return plural_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'equation'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_singular('equations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos usar las dos funciones anteriores para devolver una lista de sustantivos cambiados en plural o singular, dependiendo del sustantivo original. El siguiente código usa la función is_plural_wn para determinar si el sustantivo es plural. También puede usar la función is_plural_nltk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plurals_wn(words_with_pos):\n",
    "    other_nouns = []\n",
    "    for noun_info in words_with_pos:\n",
    "        word = noun_info[0]\n",
    "        plural = is_plural_wn(word)\n",
    "        if (plural):\n",
    "            singular = get_singular(word)\n",
    "            other_nouns.append(singular)\n",
    "        else:\n",
    "            plural = get_plural(word)\n",
    "            other_nouns.append(plural)\n",
    "    \n",
    "    return other_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arrays', 'simulations', 'models', 'environments', 'papers', 'models', 'circuits', 'equation', 'photovoltaics', 'cell', 'effect', 'irradiations', 'temperatures', 'change', 'models', 'dcs', 'loads', 'loads', 'inverters', 'validations', 'study', 'loads', 'circuit', 'result']\n"
     ]
    }
   ],
   "source": [
    "# Utilice la función anterior para devolver una lista de sustantivos modificados:\n",
    "other_nouns_wn = plurals_wn(nouns)\n",
    "print(other_nouns_wn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obteniendo análisis de dependencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librería\n",
    "import spacy\n",
    "\n",
    "\n",
    "'''\n",
    "Recurrir al reporte de errores\n",
    "'''\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargando la frase que se va a analizar\n",
    "sentence = 'I have seldom heard him mention her under any other name.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargando el motor spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesesando la oración con el motor spacy\n",
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La información de dependencias estará contenida en el objeto doc. Podemos ver las etiquetas de dependencias haciendo un bucle a través de los tokens en doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t nsubj \t nominal subject\n",
      "have \t aux \t auxiliary\n",
      "seldom \t advmod \t adverbial modifier\n",
      "heard \t ROOT \t None\n",
      "him \t nsubj \t nominal subject\n",
      "mention \t ccomp \t clausal complement\n",
      "her \t dobj \t direct object\n",
      "under \t prep \t prepositional modifier\n",
      "any \t det \t determiner\n",
      "other \t amod \t adjectival modifier\n",
      "name \t pobj \t object of preposition\n",
      ". \t punct \t punctuation\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, \"\\t\", token.dep_, \"\\t\", spacy.explain(token.dep_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para explorar la estructura de análisis de dependencias, podemos usar los atributos de la clase Token. Usando sus atributos **ancestors** e **children**, podemos obtener los tokens de los que depende este token y los tokens que dependen de él, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "['heard']\n",
      "have\n",
      "['heard']\n",
      "seldom\n",
      "['heard']\n",
      "heard\n",
      "[]\n",
      "him\n",
      "['mention', 'heard']\n",
      "mention\n",
      "['heard']\n",
      "her\n",
      "['mention', 'heard']\n",
      "under\n",
      "['mention', 'heard']\n",
      "any\n",
      "['name', 'under', 'mention', 'heard']\n",
      "other\n",
      "['name', 'under', 'mention', 'heard']\n",
      "name\n",
      "['under', 'mention', 'heard']\n",
      ".\n",
      "['heard']\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)\n",
    "    ancestors = [t.text for t in token.ancestors]\n",
    "    print(ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "[]\n",
      "have\n",
      "[]\n",
      "seldom\n",
      "[]\n",
      "heard\n",
      "['I', 'have', 'seldom', 'mention', '.']\n",
      "him\n",
      "[]\n",
      "mention\n",
      "['him', 'her', 'under']\n",
      "her\n",
      "[]\n",
      "under\n",
      "['name']\n",
      "any\n",
      "[]\n",
      "other\n",
      "[]\n",
      "name\n",
      "['any', 'other']\n",
      ".\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# tokens secundarios\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "    children = [t.text for t in token.children]\n",
    "    print(children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "['I']\n",
      "have\n",
      "['have']\n",
      "seldom\n",
      "['seldom']\n",
      "heard\n",
      "['I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', '.']\n",
      "him\n",
      "['him']\n",
      "mention\n",
      "['him', 'mention', 'her', 'under', 'any', 'other', 'name']\n",
      "her\n",
      "['her']\n",
      "under\n",
      "['under', 'any', 'other', 'name']\n",
      "any\n",
      "['any']\n",
      "other\n",
      "['other']\n",
      "name\n",
      "['any', 'other', 'name']\n",
      ".\n",
      "['.']\n"
     ]
    }
   ],
   "source": [
    "# También podemos ver el subárbol en el que está el token:\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "    subtree = [t.text for t in token.subtree]\n",
    "    print(subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probemos con español**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python -m spacy download es_core_news_sm\n",
    "'''\n",
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Me gusta la programación y el mundo maker.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Me gusta la programación y el mundo maker."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(sentence)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me \t obj \t object\n",
      "gusta \t ROOT \t None\n",
      "la \t det \t determiner\n",
      "programación \t nsubj \t nominal subject\n",
      "y \t cc \t coordinating conjunction\n",
      "el \t det \t determiner\n",
      "mundo \t conj \t conjunct\n",
      "maker \t amod \t adjectival modifier\n",
      ". \t punct \t punctuation\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, \"\\t\", token.dep_, \"\\t\", spacy.explain(token.dep_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me\n",
      "['gusta']\n",
      "gusta\n",
      "[]\n",
      "la\n",
      "['programación', 'gusta']\n",
      "programación\n",
      "['gusta']\n",
      "y\n",
      "['mundo', 'programación', 'gusta']\n",
      "el\n",
      "['mundo', 'programación', 'gusta']\n",
      "mundo\n",
      "['programación', 'gusta']\n",
      "maker\n",
      "['mundo', 'programación', 'gusta']\n",
      ".\n",
      "['gusta']\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)\n",
    "    ancestors = [t.text for t in token.ancestors]\n",
    "    print(ancestors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir la frase en cláusulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando trabajamos con texto, frecuentemente tratamos con oraciones compuestas (oraciones con dos partes que son igualmente importantes) y complejas (oraciones con una parte dependiendo de otra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la librería\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el motor de spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la oración\n",
    "sentence = \"He eats cheese, but he won't eat ice cream.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamos la oración con el motor:\n",
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He \t 0 \t PRON \t nsubj \t ['eats'] \t []\n",
      "eats \t 1 \t VERB \t ROOT \t [] \t ['He', 'cheese', ',', 'but', 'eat']\n",
      "cheese \t 2 \t NOUN \t dobj \t ['eats'] \t []\n",
      ", \t 3 \t PUNCT \t punct \t ['eats'] \t []\n",
      "but \t 4 \t CCONJ \t cc \t ['eats'] \t []\n",
      "he \t 5 \t PRON \t nsubj \t ['eat', 'eats'] \t []\n",
      "wo \t 6 \t VERB \t aux \t ['eat', 'eats'] \t []\n",
      "n't \t 7 \t PART \t neg \t ['eat', 'eats'] \t []\n",
      "eat \t 8 \t VERB \t conj \t ['eats'] \t ['he', 'wo', \"n't\", 'cream', '.']\n",
      "ice \t 9 \t NOUN \t compound \t ['cream', 'eat', 'eats'] \t []\n",
      "cream \t 10 \t NOUN \t dobj \t ['eat', 'eats'] \t ['ice']\n",
      ". \t 11 \t PUNCT \t punct \t ['eat', 'eats'] \t []\n"
     ]
    }
   ],
   "source": [
    "# Poniendo atención a la estructura de la oración\n",
    "for token in doc:\n",
    "    ancestors = [t.text for t in token.ancestors]\n",
    "    children = [t.text for t in token.children]\n",
    "    print(token.text, \"\\t\", token.i, \"\\t\", token.pos_, \"\\t\", token.dep_, \"\\t\", \n",
    " ancestors, \"\\t\", children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos la siguiente función para encontrar el token raíz de la oración, que suele ser el verbo principal. En los casos en que hay una cláusula dependiente, es el verbo de la cláusula independiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root_of_sentence(doc):\n",
    "    root_token = None\n",
    "    for token in doc:\n",
    "        if (token.dep_ == \"ROOT\"):\n",
    "            root_token = token\n",
    "    return root_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eats"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encontramos el símbolo raíz de la frase:\n",
    "root_token = find_root_of_sentence(doc)\n",
    "root_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora podemos usar la siguiente función para encontrar \n",
    "## los otros verbos en la oración:\n",
    "def find_other_verbs(doc, root_token):\n",
    "    other_verbs = []\n",
    "    for token in doc:\n",
    "        ancestors = list(token.ancestors)\n",
    "        if (token.pos_ == \"VERB\" and len(ancestors) == 1\\\n",
    "            and ancestors[0] == root_token):\n",
    "            other_verbs.append(token)\n",
    "    return other_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[eat]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utilice la función anterior para encontrar los verbos \n",
    "## restantes en la oración:\n",
    "other_verbs = find_other_verbs(doc, root_token)\n",
    "other_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buscamos los \"token spans\" de cada verbo:\n",
    "def get_clause_token_span_for_verb(verb, doc, all_verbs):\n",
    "    first_token_index = len(doc)\n",
    "    last_token_index = 0\n",
    "    this_verb_children = list(verb.children)\n",
    "    for child in this_verb_children:\n",
    "        if (child not in all_verbs):\n",
    "            if (child.i < first_token_index):\n",
    "                first_token_index = child.i\n",
    "            if (child.i > last_token_index):\n",
    "                last_token_index = child.i\n",
    "    return(first_token_index, last_token_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuniremos todos los verbos en una matriz y procesaremos cada uno usando la función precedente. Esto devolverá una tupla de índices de inicio y fin para la cláusula de cada verbo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_spans = []\n",
    "all_verbs = [root_token] + other_verbs\n",
    "for other_verb in all_verbs:\n",
    "    (first_token_index, last_token_index) = \\\n",
    "    get_clause_token_span_for_verb(other_verb,doc, all_verbs)\n",
    "    token_spans.append((first_token_index, last_token_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[He eats cheese,, he won't eat ice cream]\n"
     ]
    }
   ],
   "source": [
    "# Armamos los rangos de tokens para cada cláusula:\n",
    "sentence_clauses = []\n",
    "for token_span in token_spans:\n",
    "    start = token_span[0]\n",
    "    end = token_span[1]\n",
    "    if (start < end):\n",
    "        clause = doc[start:end]\n",
    "        sentence_clauses.append(clause)\n",
    "sentence_clauses = sorted(sentence_clauses,\n",
    "                          key=lambda tup: tup[0])\n",
    "print(sentence_clauses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He eats cheese,', \"he won't eat ice cream\"]\n"
     ]
    }
   ],
   "source": [
    "# Imprimimos el resultado final:\n",
    "clauses_text = [clause.text for clause in sentence_clauses]\n",
    "print(clauses_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de trozos sustantivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**noun phrases** Los trozos sustantivos se conocen en la lingüística como frases sustantivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el texto usando la siguiente función:\n",
    "def read_text_file(filename):\n",
    "    file = open(filename, \"r\", encoding=\"utf-8\") \n",
    "    return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el texto:\n",
    "text = read_text_file(\"pv.txt\")\n",
    "\n",
    "# Reemplazando los saltos de línea con espacios:\n",
    "text = text.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A photovoltaic array (PVA) simulation model to be used in Matlab-Simulink GUI environment is developed and presented in this paper. The model is developed using basic circuit equations of the photovoltaic (PV) solar cells including the effects of solar irradiation and temperature changes. The new model was tested using a directly coupled dc load as well as ac load via an inverter. Test and validation studies with proper load matching circuits are simulated and results are presented here."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializamos el motor Spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A photovoltaic array (PVA) simulation model\n",
      "Matlab-Simulink GUI environment\n",
      "this paper\n",
      "The model\n",
      "basic circuit equations\n",
      "the photovoltaic (PV) solar cells\n",
      "the effects\n",
      "solar irradiation and temperature changes\n",
      "The new model\n",
      "a directly coupled dc load\n",
      "ac load\n",
      "an inverter\n",
      "Test and validation studies\n",
      "proper load matching circuits\n",
      "results\n"
     ]
    }
   ],
   "source": [
    "# Los fragmentos sustantivos están contenidos en la\n",
    "## variable de clase doc.noun_chunks. \n",
    "### Podemos imprimir los trozos\n",
    "for noun_chunk in doc.noun_chunks:\n",
    "    print(noun_chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hay más**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los fragmentos sustantivos son objetos spaCy Span y tienen todas sus propiedades. Ver el sitio oficial \n",
    "documentación en https://spacy.io/api/token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos Spacy\n",
    "import spacy\n",
    "# Cargamos el motor:\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una oración de ejemplo:\n",
    "sentence = \"All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamos la oración con el motor:\n",
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All emotions\n",
      "one\n",
      "his cold, precise but admirably balanced mind\n"
     ]
    }
   ],
   "source": [
    "#Veamos los fragmentos sustantivos de esta frase:\n",
    "for noun_chunk in doc.noun_chunks:\n",
    "    print(noun_chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All emotions \t 0 \t 2\n",
      "one \t 5 \t 6\n",
      "his cold, precise but admirably balanced mind \t 11 \t 19\n"
     ]
    }
   ],
   "source": [
    "for noun_chunk in doc.noun_chunks:\n",
    "    print(noun_chunk.text, \"\\t\", noun_chunk.start, \"\\t\", noun_chunk.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All emotions \t All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.\n",
      "one \t All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.\n",
      "his cold, precise but admirably balanced mind \t All emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind.\n"
     ]
    }
   ],
   "source": [
    "# También podemos imprimir la frase donde pertenece el sustantivo chunk:\n",
    "for noun_chunk in doc.noun_chunks:\n",
    "    print(noun_chunk.text, \"\\t\", noun_chunk.sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All emotions \t emotions\n",
      "one \t one\n",
      "his cold, precise but admirably balanced mind \t mind\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Al igual que una oración, cualquier fragmento sustantivo incluye una raíz, \n",
    "que es el token del que dependen todos los demás tokens. \n",
    "En una frase sustantivo, ese es el sustantivo:\n",
    "'''\n",
    "for noun_chunk in doc.noun_chunks:\n",
    "    print(noun_chunk.text, \"\\t\", noun_chunk.root.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra propiedad muy útil de **Span** es **similirity**, que es la similitud semántica de diferentes textos. Vamos a probarlo. Vamos a cargar otro sustantivo **emotions**, y procesarlo usando **spacy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_span = \"emotions\"\n",
    "other_doc = nlp(other_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44171491123509243\n",
      "-0.10580340746622595\n",
      "0.022172331323610288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cdani\\anaconda3\\envs\\nlp_book\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for noun_chunk in doc.noun_chunks:\n",
    "    print(noun_chunk.similarity(other_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022172331323610288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cdani\\anaconda3\\envs\\nlp_book\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Corregimos el aviso anterior:\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "print(noun_chunk.similarity(other_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entidades extractoras y relaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias\n",
    "import spacy\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la función que vamos a usar\n",
    "def find_root_of_sentence(doc):\n",
    "    root_token = None\n",
    "    for token in doc:\n",
    "        if (token.dep_ == \"ROOT\"):\n",
    "            root_token = token\n",
    "    return root_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el motor de spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las oraciones que vamos a procesar\n",
    "sentences = [\"All living things are made of cells.\",  \"Cells have organelles.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_patterns = [[{\"POS\":\"AUX\"}, {\"POS\":\"VERB\"},  {\"POS\":\"ADP\"}],  [{\"POS\":\"AUX\"}]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función **contains_root** comprueba si una frase verbal contiene la raíz de la frase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_root(verb_phrase, root):\n",
    "    vp_start = verb_phrase.start\n",
    "    vp_end = verb_phrase.end\n",
    "    if (root.i >= vp_start and root.i <= vp_end):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función **get_verb_phrases** obtiene las frases verbales de un objeto spaCy Doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verb_phrases(doc):\n",
    "    root = find_root_of_sentence(doc)\n",
    "    verb_phrases = textacy.extract.matches(doc, verb_patterns)\n",
    "    new_vps = []\n",
    "    for verb_phrase in verb_phrases:\n",
    "        if (contains_root(verb_phrase, root)):\n",
    "            new_vps.append(verb_phrase)\n",
    "            return new_vps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función **longer_verb_phrase** encuentra la frase verbal más larga:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longer_verb_phrase(verb_phrases):\n",
    "    longest_length = 0\n",
    "    longest_verb_phrase = None\n",
    "    for verb_phrase in verb_phrases:\n",
    "        if len(verb_phrase) > longest_length:\n",
    "            longest_verb_phrase = verb_phrase\n",
    "            return longest_verb_phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función **find_noun_phrase** buscará frases de sustantivo en el lado izquierdo o derecho de la frase del verbo principal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_noun_phrase(verb_phrase, noun_phrases, side):\n",
    "    for noun_phrase in noun_phrases:\n",
    "        if (side == \"left\" and noun_phrase.start < verb_phrase.start):\n",
    "            return noun_phrase\n",
    "        elif (side == \"right\" and noun_phrase.start > verb_phrase.start):\n",
    "            return noun_phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta función, utilizaremos las funciones anteriores para encontrar trillizos de objeto-relación en las oraciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_triplet(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    verb_phrases = get_verb_phrases(doc)\n",
    "    noun_phrases = doc.noun_chunks\n",
    "    verb_phrase = None\n",
    "    if (len(verb_phrases) > 1):\n",
    "        verb_phrase = longer_verb_phrase(list(verb_phrases))\n",
    "    else:\n",
    "        verb_phrase = verb_phrases[0]\n",
    "        left_noun_phrase = find_noun_phrase(verb_phrase, noun_phrases, \"left\")\n",
    "        right_noun_phrase = find_noun_phrase(verb_phrase, noun_phrases, \"right\")\n",
    "    return (left_noun_phrase, verb_phrase, right_noun_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'textacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-b68bd97602fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Usamos un bucle para encontrar la relación trillizos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mleft_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_np\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_triplet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_np\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-03e27ecab595>\u001b[0m in \u001b[0;36mfind_triplet\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_triplet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mverb_phrases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_verb_phrases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mnoun_phrases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoun_chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mverb_phrase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-73a215472038>\u001b[0m in \u001b[0;36mget_verb_phrases\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_verb_phrases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_root_of_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mverb_phrases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverb_patterns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mnew_vps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mverb_phrase\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mverb_phrases\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'textacy' is not defined"
     ]
    }
   ],
   "source": [
    "# Usamos un bucle para encontrar la relación trillizos\n",
    "for sentence in sentences:\n",
    "    (left_np, vp, right_np) = find_triplet(sentence)\n",
    "    print(left_np, \"\\t\", vp, \"\\t\", right_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
